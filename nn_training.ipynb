{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18317106-68d2-495f-af33-764fa228cdb0",
   "metadata": {},
   "source": [
    "## Neural Network training\n",
    "\n",
    "\n",
    "We have prepared for you an extract of 3000 variant images for Genome in a Bottle NA12878 sample.\n",
    "Half images are high-quality GiB variants, half images are sequencing artifacts.\n",
    "The chosen variants are uniformly distributed on the whole genome length.\n",
    "\n",
    "Having all this data, we are ready to train the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e1007a-e6dd-4c6d-921c-432f4ce288f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb38f10b-289f-4831-9c7a-a1f2a5369a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!cd {DATASET_DIR}; find . -name '*.img' -printf \"%P\\n\" > {FILE_LIST} #generate the list of all variant images in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67750d1e-5fc8-45f1-a6eb-dc91bddbd4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ARCHIVE_PATH = 'data/dataset/tutorial_GiB.zip'\n",
    "\n",
    "IMAGES_LIST = 'data/dataset/all_images_list' #relative paths to all images in the dataset archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e30d01e2-29db-46c5-8132-fee14e3c7654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "germline/chr4_159690256.img\n",
      "germline/chr3_109169659.img\n",
      "germline/chr5_125338322.img\n",
      "germline/chr7_134703331.img\n",
      "germline/chr21_40192789.img\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 {IMAGES_LIST}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "622c68d8-43ba-40bd-813a-638a2baae358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>germline/chr4_159690256.img</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>germline/chr3_109169659.img</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>germline/chr5_125338322.img</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>germline/chr7_134703331.img</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>germline/chr21_40192789.img</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 relative_path\n",
       "0  germline/chr4_159690256.img\n",
       "1  germline/chr3_109169659.img\n",
       "2  germline/chr5_125338322.img\n",
       "3  germline/chr7_134703331.img\n",
       "4  germline/chr21_40192789.img"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df = pd.read_csv(IMAGES_LIST, names=['relative_path']) # list of all images as pandas dataframe\n",
    "dataset_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb2b777-bdb9-4ce1-8125-388e8b8c7899",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> Now we need to add an extra column which should be 1 for germline variants and 0 for sequencing artifacts. How would one implement this? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "267b4ffe-19a3-4b1e-8084-30b81a5d6016",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative_path</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>germline/chr4_159690256.img</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>germline/chr3_109169659.img</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>germline/chr5_125338322.img</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>germline/chr7_134703331.img</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>germline/chr21_40192789.img</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 relative_path  labels\n",
       "0  germline/chr4_159690256.img       1\n",
       "1  germline/chr3_109169659.img       1\n",
       "2  germline/chr5_125338322.img       1\n",
       "3  germline/chr7_134703331.img       1\n",
       "4  germline/chr21_40192789.img       1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df['labels'] = dataset_df.apply(lambda x:int(x['relative_path'].startswith('germline')),axis=1) #label=1 if relative_path begins with 'germline'\n",
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d25770-b9d9-42e4-9457-297757a70619",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> Now let's split our dataset_df into train_df and val_df s.t. 90% of data goes to the train_df and the remaining 90% goes to val_df. How can this be implemented?  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33ba6e44-6ef2-4dae-b9ae-c6c20327b4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=9, shuffle=True, random_state=1) #9 splits --> 10% of examples goes to validation set\n",
    "\n",
    "placeholder = np.ones((len(dataset_df),1)) #placeholder of length equal to the number of samples\n",
    "\n",
    "images_labels = dataset_df['labels'].tolist()\n",
    "\n",
    "skf.get_n_splits(placeholder, images_labels)\n",
    "\n",
    "idx_train, idx_test = list(skf.split(placeholder, images_labels))[0] #take the 0th fold\n",
    "\n",
    "train_df = dataset_df.iloc[idx_train] \n",
    "\n",
    "val_df = dataset_df.iloc[idx_test] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce4077dd-efb9-40ae-af3f-30eb0e4386a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataframe class counts:\n",
      "0    1333\n",
      "1    1333\n",
      "Name: labels, dtype: int64\n",
      "\n",
      "Validation dataframe class counts:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    167\n",
       "1    167\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print the class distribution in train and validation datasets\n",
    "\n",
    "print('Train dataframe class counts:')\n",
    "print(train_df['labels'].value_counts())\n",
    "print()\n",
    "print('Validation dataframe class counts:')\n",
    "val_df['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5530c28c-d9e7-4003-986f-3237ef019ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative_path</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>germline/chr5_125338322.img</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>germline/chr7_134703331.img</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>germline/chr21_40192789.img</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>germline/chr2_126513701.img</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>germline/chr18_63578873.img</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2995</th>\n",
       "      <td>negative/chr13_79255988.img</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2996</th>\n",
       "      <td>negative/chr12_123816211.img</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2997</th>\n",
       "      <td>negative/chr16_47496816.img</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2998</th>\n",
       "      <td>negative/chrX_140444033.img</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2999</th>\n",
       "      <td>negative/chr3_56194663.img</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2666 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     relative_path  labels\n",
       "2      germline/chr5_125338322.img       1\n",
       "3      germline/chr7_134703331.img       1\n",
       "4      germline/chr21_40192789.img       1\n",
       "5      germline/chr2_126513701.img       1\n",
       "6      germline/chr18_63578873.img       1\n",
       "...                            ...     ...\n",
       "2995   negative/chr13_79255988.img       0\n",
       "2996  negative/chr12_123816211.img       0\n",
       "2997   negative/chr16_47496816.img       0\n",
       "2998   negative/chrX_140444033.img       0\n",
       "2999    negative/chr3_56194663.img       0\n",
       "\n",
       "[2666 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667c9d88-a92b-48ed-be46-89da1341a63b",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> What's wrong with our train dataframe? How to change this? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6cd1d191-7a06-4290-9f7f-fa063a4c10d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relative_path</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>negative/chr2_235329604.img</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312</th>\n",
       "      <td>germline/chr2_208500922.img</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>698</th>\n",
       "      <td>germline/chr2_107581433.img</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2056</th>\n",
       "      <td>negative/chr17_59015327.img</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2207</th>\n",
       "      <td>negative/chr7_142470336.img</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    relative_path  labels\n",
       "1697  negative/chr2_235329604.img       0\n",
       "312   germline/chr2_208500922.img       1\n",
       "698   germline/chr2_107581433.img       1\n",
       "2056  negative/chr17_59015327.img       0\n",
       "2207  negative/chr7_142470336.img       0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.sample(frac=1, random_state=1)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acd92fc7-fcc7-44d6-a62f-b908dff05751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "\n",
    "import pickle\n",
    "import zipfile\n",
    "\n",
    "dataset_archive = zipfile.ZipFile(DATASET_ARCHIVE_PATH, 'r') #open the archive with all variant images in the dataset\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    \n",
    "    '''\n",
    "    Dataset of variant images\n",
    "    '''\n",
    "\n",
    "    def __init__(self, \n",
    "                 data,           #relative path to images with corresponding labels\n",
    "                 target_height,  #target image height for the neural network\n",
    "                ):\n",
    "\n",
    "        self.data = data\n",
    "        self.target_height = target_height\n",
    "        \n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        '''\n",
    "        Retrive an image\n",
    "                \n",
    "        If image height is smaller than self.target_height,\n",
    "        we pad it with \"N\" to reach the required self.target_height.\n",
    "        If image height is larger that self.target_height,\n",
    "        we remove some reads on the top and on the bottom, leaving the central part.\n",
    "        \n",
    "        The original image is then shifted to the center of this new full image.\n",
    "        '''\n",
    "        \n",
    "        relative_image_path, label = self.data[idx]\n",
    "        \n",
    "        #full_image_path = self.dataset_dir+relative_image_path\n",
    "        #with open(full_image_path, 'rb') as imgfile:\n",
    "        #        image = pickle.load(imgfile)\n",
    "\n",
    "        imgfile = dataset_archive.open(relative_image_path) #open image file directly out of the archive\n",
    "        image = pickle.load(imgfile)                #load pickle data from the image file\n",
    "            \n",
    "        one_hot_ref = image['one_hot_ref']      #one-hot encoding of reference bases\n",
    "        p_hot_reads = image['p_hot_reads']*1e-4 #p-hot encoding of reads bases\n",
    "        flags_reads = image['flags_reads']      #flags for reads\n",
    "            \n",
    "        image_height, image_width, _ = p_hot_reads.shape\n",
    "                  \n",
    "        one_hot_ref = np.tile(one_hot_ref, (image_height,1,1)) #propagate reference bases over all reads\n",
    "            \n",
    "        image = np.concatenate((one_hot_ref,p_hot_reads,flags_reads), axis=2)\n",
    "\n",
    "        if self.target_height>image_height:\n",
    "            #pad image with 'N' to reach the target height\n",
    "            padding_image = np.concatenate(\n",
    "                    (np.tile(one_hot_ref[0,:,:],(self.target_height-image_height,1,1)), #4 channels for reference bases, should be the same as in the reads\n",
    "                    np.ones((self.target_height-image_height, image_width, 4))*0.25,    #p-hot encoding for 'N'\n",
    "                    np.zeros((self.target_height-image_height, image_width, 6))         #read flags\n",
    "                    ), \n",
    "                    axis=2)\n",
    "            \n",
    "            full_image = np.concatenate((image, padding_image), axis = 0) #concatenate over the reads axis\n",
    "            full_image = np.roll(full_image,max(self.target_height//2-image_height//2,0),axis=0) #put the piledup reads in the center of image\n",
    "\n",
    "        else:\n",
    "            #if there are too many reads, keep reads in the center, remove at the top and at the bottom\n",
    "            shift = max(image_height//2-self.target_height//2,0)\n",
    "            full_image = image[shift:shift+self.target_height,:,:]\n",
    "                \n",
    "        full_image = np.transpose(full_image, (2,0,1)) #change dimensions order to CxWxH\n",
    "\n",
    "        return full_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "55380343-ef58-44b6-9106-7a838966b4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define train and validation datasets\n",
    "\n",
    "train_dataset = Dataset(train_df.values.tolist(), target_height=70)\n",
    "\n",
    "val_dataset = Dataset(val_df.values.tolist(), target_height=70)\n",
    "\n",
    "#define train and validation dataloaders\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=96, shuffle=False, num_workers=0)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=96, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bf05595-4e04-4802-87a2-d5297028ef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the neural network architecture\n",
    "\n",
    "class ConvNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, dropout=0.):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        #150x70\n",
    "\n",
    "        self.conv1 = nn.Conv2d(14, 32, kernel_size=5, stride=1, padding=0) #146x66\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.dp1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=0) #142x62\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.dp2 = nn.Dropout(dropout)\n",
    "        self.mp2 = nn.MaxPool2d(kernel_size=2, stride=2)#71x31x64\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=0)#67x27\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.dp3 = nn.Dropout(dropout)\n",
    "        self.mp3 = nn.MaxPool2d(kernel_size=2, stride=2)#33x13\n",
    "\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=5, stride=1, padding=0)#29x9\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.act4 = nn.ReLU()\n",
    "        self.dp4 = nn.Dropout(dropout)\n",
    "        self.mp4 = nn.MaxPool2d(kernel_size=2, stride=2)#14x4\n",
    "\n",
    "        self.flt = nn.Flatten()\n",
    "\n",
    "        self.fc5 = nn.Linear(14 * 4 * 64, 64)\n",
    "        self.bn5 = nn.BatchNorm1d(64)\n",
    "        self.act5 = nn.ReLU()\n",
    "        self.dp5 = nn.Dropout(dropout)\n",
    "\n",
    "        self.fc6 = nn.Linear(64, 1)\n",
    "        self.act6 = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.act1(out)\n",
    "        out = self.dp1(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.act2(out)\n",
    "        out = self.dp2(out)\n",
    "        out = self.mp2(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        out = self.act3(out)\n",
    "        out = self.dp3(out)\n",
    "        out = self.mp3(out)\n",
    "\n",
    "        out = self.conv4(out)\n",
    "        out = self.bn4(out)\n",
    "        out = self.act4(out)\n",
    "        out = self.dp4(out)\n",
    "        out = self.mp4(out)\n",
    "\n",
    "        out = self.flt(out)\n",
    "\n",
    "        out = self.fc5(out)\n",
    "        out = self.bn5(out)\n",
    "        out = self.act5(out)\n",
    "        out = self.dp5(out)\n",
    "\n",
    "        out = self.fc6(out)\n",
    "        out = self.act6(out)\n",
    "        \n",
    "        out = torch.squeeze(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96436c89-0b6f-4870-82f6-3e7904304806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CUDA device: GPU\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get access to GPU\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('\\nCUDA device: GPU\\n')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('\\nCUDA device: CPU\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6aeb462e-a132-4101-b6ea-321a7c8e7f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNN(dropout=0.2) #create the model\n",
    "\n",
    "model = model.to(device)    #model to CUDA\n",
    "\n",
    "model_params = [p for p in model.parameters() if p.requires_grad] #model parameters for optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f832d28-5184-4701-a1bf-44f8193c1d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [64, 32, 146, 66]          11,232\n",
      "       BatchNorm2d-2          [64, 32, 146, 66]              64\n",
      "              ReLU-3          [64, 32, 146, 66]               0\n",
      "           Dropout-4          [64, 32, 146, 66]               0\n",
      "            Conv2d-5          [64, 64, 142, 62]          51,264\n",
      "       BatchNorm2d-6          [64, 64, 142, 62]             128\n",
      "              ReLU-7          [64, 64, 142, 62]               0\n",
      "           Dropout-8          [64, 64, 142, 62]               0\n",
      "         MaxPool2d-9           [64, 64, 71, 31]               0\n",
      "           Conv2d-10           [64, 64, 67, 27]         102,464\n",
      "      BatchNorm2d-11           [64, 64, 67, 27]             128\n",
      "             ReLU-12           [64, 64, 67, 27]               0\n",
      "          Dropout-13           [64, 64, 67, 27]               0\n",
      "        MaxPool2d-14           [64, 64, 33, 13]               0\n",
      "           Conv2d-15            [64, 64, 29, 9]         102,464\n",
      "      BatchNorm2d-16            [64, 64, 29, 9]             128\n",
      "             ReLU-17            [64, 64, 29, 9]               0\n",
      "          Dropout-18            [64, 64, 29, 9]               0\n",
      "        MaxPool2d-19            [64, 64, 14, 4]               0\n",
      "          Flatten-20                 [64, 3584]               0\n",
      "           Linear-21                   [64, 64]         229,440\n",
      "      BatchNorm1d-22                   [64, 64]             128\n",
      "             ReLU-23                   [64, 64]               0\n",
      "          Dropout-24                   [64, 64]               0\n",
      "           Linear-25                    [64, 1]              65\n",
      "          Sigmoid-26                    [64, 1]               0\n",
      "================================================================\n",
      "Total params: 497,505\n",
      "Trainable params: 497,505\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 35.89\n",
      "Forward/backward pass size (MB): 2047.31\n",
      "Params size (MB): 1.90\n",
      "Estimated Total Size (MB): 2085.10\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#show the model architecture\n",
    "\n",
    "from torchsummary import summary\n",
    "summary(model,(14,150,70), batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0029c6f3-2ec2-4847-be52-0f67a1acd30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model_params, lr=2e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26fdf0a-b621-4e35-90d9-fc7b728b03b7",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> We shall implement a function to train the model. What loss function should we choose ? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "483cac52-8cb0-4010-bfff-b446c36f852c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2fcdc06fd8e4d258fede94b195c5431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train(model, dataloader):\n",
    "    \n",
    "    model.train() #model to train mode\n",
    "   \n",
    "    criterion = nn.BCELoss() #binary cross-entropy\n",
    "    \n",
    "    tot_itr = len(dataloader.dataset.data)//dataloader.batch_size #total train iterations\n",
    "    \n",
    "    pbar = tqdm(total = tot_itr, ncols=700) #progress bar\n",
    "    \n",
    "    beta = 0.98 #beta of running average, don't change\n",
    "    \n",
    "    avg_loss = 0. #average loss\n",
    "    \n",
    "    for itr_idx, (images, labels) in enumerate(dataloader):\n",
    "        \n",
    "        images = images.to(torch.float).to(device) #images to torch.float32 then to GPU\n",
    "        labels = labels.to(torch.float).to(device) #labels to torch.float32 then to GPU\n",
    "\n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #exponential moving evaraging of loss\n",
    "        avg_loss = beta * avg_loss + (1-beta)*loss.item()\n",
    "        smoothed_loss = avg_loss / (1 - beta**(itr_idx+1))\n",
    " \n",
    "        pbar.update(1)\n",
    "        pbar.set_description(f\"Running loss:{smoothed_loss:.4}\")\n",
    "        \n",
    "    return smoothed_loss\n",
    "        \n",
    "train(model, train_dataloader);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea8b48a7-9e6a-4027-b9e1-da332d3251a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dafc4cadc9ed4d6a8be797d9af444f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|                                                                                                         …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def validate(model, dataloader):\n",
    "    \n",
    "    model.eval() #model to validation mode\n",
    "   \n",
    "    criterion = nn.BCELoss() #binary cross-entropy\n",
    "    \n",
    "    tot_itr = len(dataloader.dataset.data)//dataloader.batch_size #total validation iterations\n",
    "    \n",
    "    pbar = tqdm(total = tot_itr, ncols=700) #progress bar\n",
    "    \n",
    "    all_loss = 0. #all losses, for simple averaging\n",
    "    \n",
    "    all_preds = [] #all validation predictions\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        for itr_idx, (images, labels) in enumerate(dataloader):\n",
    "        \n",
    "            images = images.to(torch.float).to(device) #images to torch.float32 then to GPU\n",
    "            labels = labels.to(torch.float).to(device) #labels to torch.float32 then to GPU\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels) \n",
    "\n",
    "            all_loss += loss.item()\n",
    "            \n",
    "            all_preds.extend(list(zip(outputs.cpu().numpy(), labels.cpu().numpy())))\n",
    "             \n",
    "            pbar.update(1)\n",
    "            pbar.set_description(f\"Running loss:{all_loss/(itr_idx+1):.4}\")\n",
    "        \n",
    "    return all_loss/(itr_idx+1), all_preds #return average loss and predictions\n",
    "\n",
    "val_loss, val_preds = validate(model, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2d88cc-58f4-4df0-a0f7-5db845854c2e",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> By the way, why do we use exponential moving averaging for training and simple aberaging during validation ? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "029cff6a-8cd7-4da7-a1ca-f960843c97cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximal f1-score 0.83 is reached with precision 0.82 and recall 0.84\n"
     ]
    }
   ],
   "source": [
    "#Compute precision-recall curve and get the maximum f1-score\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "    \n",
    "outputs, labels = list(zip(*val_preds))\n",
    "\n",
    "all_precision, all_recall, thresholds = precision_recall_curve(labels, outputs)\n",
    "\n",
    "f1 = 2*all_precision*all_recall/(all_precision+all_recall+1e-10)\n",
    "\n",
    "best_f1 = max(f1)\n",
    "\n",
    "best_f1_index = np.nanargmax(f1) #index of the maximal f1-score to get the corresponding precision and recall\n",
    "\n",
    "print(f'Maximal f1-score {best_f1:.2} is reached with precision {all_precision[best_f1_index]:.2} and recall {all_recall[best_f1_index]:.2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5896b762-2e70-48c0-996e-e5e21c7b7727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot precision-recall curve\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_recall, all_precision, 'r-', lw=2, marker = '.', markersize = 5)\n",
    "plt.grid()\n",
    "plt.ylabel('Precision', fontsize=15)\n",
    "plt.xlabel('Recall', fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ca43ae-af51-4f8c-858c-75d6e2d5fe5b",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> In our train set we had the class proportion true:false = 1:1. For an actual GiB NA12878 sample the class proportion is true:false = 1:2. Do we have to do anything with this class imbalance and if we do then what? </span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
