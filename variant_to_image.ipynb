{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode variant as an image\n",
    "\n",
    "To train the Neural Network, we must encode information about each candidate variant. Relevant data will be composed of refererence and read sequences, read quality scores, read flags. We shall encode the relevant information of each variant from the VCF file using a 14-channel image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To encode variant data, we need a VCF file with variants, corresponding BAM file with aligned reads and the FASTA file of the reference genome. We prepared three toy files, with some variants withdrawn from the latest Genome in a Bottle release v. 3.3.2.\n",
    "\n",
    "Initial high-confidence VCF file: https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/NA12878_HG001/NISTv3.3.2/GRCh38/HG001_GRCh38_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer.vcf.gz\n",
    "\n",
    "Initial BAM file:  https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/NA12878/10Xgenomics_ChromiumGenome_LongRanger2.1_09302016/NA12878_GRCh38/NA12878_GRCh38_phased_possorted_bam.bam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pysam #python library to read VCF(variant calls) and SAM(BAM) (aligned reads) files\n",
    "\n",
    "import pysam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get tutorial data\n",
    "\n",
    "!wget -O data.tar 'https://drive.google.com/uc?export=download&id=1aBsEZ_I8JEKG0hL82xAnnWdAFW34Owhp'\n",
    "!tar -xvf data.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('utils/')\n",
    "from variant_to_image_helpers import * #some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#toy reference FASTA files\n",
    "ref_fasta_file = 'data/raw/GRCh38_extract.fa'\n",
    "#toy experiment BAM file\n",
    "bam_file = 'data/raw/NA12878_GRCh38_phased_possorted_extract.bam'\n",
    "#toy VCF file with candidate variants\n",
    "vcf_file = 'data/raw/HG001_GRCh38_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_first_ten.vcf.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall open the VCF file and take a look at the first 10 variants  using the pysam fetch() method. We will then make an image for the last (10th) variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vcf = pysam.VariantFile(vcf_file)\n",
    "\n",
    "variants_itr = vcf.fetch() #iterator over all variants in the VCF\n",
    "\n",
    "#display the first 10 variants in the VCF\n",
    "\n",
    "for i in range(10):\n",
    "    variant = next(variants_itr)\n",
    "    variant = dict(chrom=variant.chrom, pos=variant.pos, ref=variant.ref, alt=variant.alts[0]) #VariantRecord to dict\n",
    "    print(f\"CHROM: {variant['chrom']}, POS: {variant['pos']} REF: {variant['ref']} ALT: {variant['alt']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> Now we will open the BAM file to collect all reads around the candidate variant position. We can already exclude some reads on-the-fly by filtering using some SAM flags. Which flags can we use? See specification of the SAM format here: https://samtools.github.io/hts-specs/SAMv1.pdf </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUDE_FLAGS = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> We also need a function that convert the ANCII quality symbols to the probability that the base is called INCORRECTLY.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phred_qual(qual_symbol):\n",
    "    '''\n",
    "    Return the probability that the base is called incorrectly\n",
    "    \n",
    "    Input: ANCII quality character\n",
    "    Output: Probability\n",
    "    '''\n",
    "    \n",
    "    probability = ...\n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bam = pysam.AlignmentFile(bam_file, \"rb\" ) \n",
    "\n",
    "#collect all the reads around the candidate variant position\n",
    "\n",
    "raw_reads = []\n",
    "\n",
    "for read in bam.fetch(variant['chrom'], variant['pos']-1, variant['pos']+1):\n",
    "    \n",
    "    if (read.pos<=variant['pos']-1 and read.pos+read.rlen>variant['pos']-1 and \n",
    "        read.flag&EXCLUDE_FLAGS==0 \n",
    "       ):\n",
    "            \n",
    "            qual = np.array([get_phred_qual(q) for q in read.qual])     #probability that the base is called INCORRECTLY\n",
    "            qual = 1.-qual                                              #probability that the base is called CORRECTLY\n",
    "            \n",
    "            #for each read save absolute position, sequence, quality scores, flags and CIGAR string\n",
    "            raw_reads.append((read.pos,read.seq,qual,read.flag,read.cigartuples)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = 70 #image height\n",
    "\n",
    "IMAGE_WIDTH = 150 # image width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get reference bases around the variant position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the original reference sequence from the fasta file, variant being roughly in the center\n",
    "\n",
    "variant_chromosome = variant['chrom']\n",
    "\n",
    "variant_position = variant['pos']\n",
    "\n",
    "variant_column = IMAGE_WIDTH//2 #variant column index\n",
    "\n",
    "#ref_fasta = pysam.FastaFile(ref_fasta_file) #normal pysam FASTA reader\n",
    "ref_fasta = FastaFile(ref_fasta_file)        #simulated FASTA reader, for this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> We can again use the pysam fetch() method to get the references bases.\n",
    "How shall we get a sequence of IMAGE_WIDTH reference bases s.t. the variant position is roughly at the center? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_bases = ref_fasta.fetch(...) \n",
    "\n",
    "ref_bases = list(ref_bases)\n",
    "\n",
    "print(''.join(ref_bases)) #list to string\n",
    "\n",
    "ref_bases = np.array(list(map(encode_bases,ref_bases))) # letters to digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what happens when we take the reads as they are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_reads = len(raw_reads)\n",
    "\n",
    "reads_im = np.zeros((N_reads,IMAGE_WIDTH,1)) #pileup image of all reads centered at the variant\n",
    "\n",
    "for read_idx,read in enumerate(raw_reads):\n",
    "    \n",
    "    pos,seq = read[:2] #absolute position and sequence of the read\n",
    "    \n",
    "    seq = encode_bases(seq) # read sequence encoding: letters to digits  \n",
    "    \n",
    "    start_pos = pos-(variant['pos']-1-variant_column) #relative position of the read in the pileup image\n",
    "    \n",
    "    if start_pos<0: seq = seq[-start_pos:] #reject the data that's beyond the left image edge\n",
    "        \n",
    "    start_pos = max(start_pos,0)\n",
    "\n",
    "    reads_im[read_idx,start_pos:start_pos+len(seq),0] = seq[:IMAGE_WIDTH-start_pos]\n",
    "    \n",
    "show_diff_text(reads_im, ref_bases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> Why do some reads look unmapped? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Align reads according to their CIGAR strings.\n",
    "\n",
    "#For each read the cigar string is analysed to place read bases correctly (taking into account clips, insertions,deletion).\n",
    "\n",
    "aligned_reads = []\n",
    "\n",
    "for read_idx, read in enumerate(raw_reads):\n",
    "    \n",
    "    pos,seq,qual,flag,cigartuples = read\n",
    "        \n",
    "    aligned_seq = []  #aligned read sequence\n",
    "    aligned_qual = [] #aligned read qualities\n",
    "    \n",
    "    seq = encode_bases(seq) # read sequence encoding: letters to digits  \n",
    "    \n",
    "    c = 0 #current position in the original (not aligned) read\n",
    "    \n",
    "    #we move along the original read sequence and make insertions/delections when necessary\n",
    "    for op in cigartuples:\n",
    "        optype, oplen = op #type and length of cigar operation\n",
    "        if optype==5:#hard clip:do nothing as it's not included in seq\n",
    "            continue\n",
    "        elif optype==4:#soft clip: exclude these positions from aligned seq as they aren't used by callers\n",
    "            c+=oplen\n",
    "        elif optype==2 or optype==3 or optype==6: #deletion or padding\n",
    "            aligned_seq.extend([encode_bases('*')]*oplen)\n",
    "            aligned_qual.extend([0]*oplen)\n",
    "        elif optype==1:#insertion\n",
    "            c+=oplen\n",
    "        else: #match or mismatch\n",
    "            aligned_seq.extend(seq[c:c+oplen])\n",
    "            aligned_qual.extend(qual[c:c+oplen])  \n",
    "            c+=oplen\n",
    "            \n",
    "    #length of aligned sequence may differ from length of the original sequence : we must make sure that we still cover the variant site\n",
    "    if  pos+len(aligned_seq)>variant['pos']-1: \n",
    "        \n",
    "        aligned_reads.append((pos,aligned_seq,aligned_qual,flag))\n",
    "            \n",
    "N_reads = len(aligned_reads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reads_im = np.zeros((N_reads,IMAGE_WIDTH,2)) # 2 channels to encode sequence and probability of each read base\n",
    "\n",
    "reads_im[:,:,0] = encode_bases('N') #bases for all reads, default: no data ('N')\n",
    "reads_im[:,:,1] = 1/4. #probability to have the corresponding basis, default:equal probability of all bases\n",
    "\n",
    "variant_column = IMAGE_WIDTH//2 \n",
    "\n",
    "for read_idx, read in enumerate(aligned_reads):\n",
    "    \n",
    "    pos,seq,qual,flag = read #absolute position, sequence, quality scores and flags of the read\n",
    "        \n",
    "    start_pos = pos-(variant['pos']-1-variant_column) #relative position of the read in the image\n",
    "    \n",
    "    if start_pos<0: #left end of the read is beyond the defined image edge\n",
    "        \n",
    "        #reject data that's beyond the image\n",
    "        seq = seq[-start_pos:]\n",
    "        qual = qual[-start_pos:]\n",
    "        start_pos = 0\n",
    "        \n",
    "    rlen = len(seq)\n",
    "    \n",
    "    reads_im[read_idx,start_pos:start_pos+rlen,0] = seq[:IMAGE_WIDTH-start_pos]\n",
    "    reads_im[read_idx,start_pos:start_pos+rlen,1] = qual[:IMAGE_WIDTH-start_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_diff_text(reads_im, ref_bases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_diff_image(reads_im, ref_bases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reads_im[:,:,0] (N_reads x width) contains sequences for all reads\n",
    "\n",
    "reads_im[:,:,1] (N_reads x width) contains probabilities for all reads\n",
    "\n",
    "we shall create an array p_hot_reads that will contain probabilities for ALL bases (ACTG) at EACH read position.\n",
    "i.e. if we read 'A' and the corresponding probability is 0.99, we assign the probability (1-0.99)/3 to all the other 3 bases.\n",
    "\n",
    "<div style=\"color:red\">  How will one fill p_hot_reads (N_reads x IMAGE_WIDTH x 4) based on reads_im? <br>\n",
    "    Examples:<br>\n",
    "    p_hot_reads[3,4,0] gives the probability that for the 3rd read in position 4 the base is 'A' <br>\n",
    "    p_hot_reads[0,1,3] gives the probability that for the 0th read in position 1 the base is 'G'\n",
    "\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_hot_reads = np.zeros((N_reads,IMAGE_WIDTH,4)) # p-hot encoding of reads probabilities: each channel gives the probability of the corresponding base (ACTG)\n",
    "\n",
    "for basis_idx in range(4): \n",
    "    #basis_idx 0:A 1:C 2:T 3:G\n",
    "    p_hot_reads[:,:,basis_idx] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_ref = np.zeros((1,IMAGE_WIDTH,4)) #one-hot encoding of reference bases, same for all reads\n",
    "\n",
    "for basis_idx in range(4):\n",
    "    one_hot_ref[:,(ref_bases==basis_idx),basis_idx] = 1.\n",
    "    \n",
    "flags_reads = np.zeros((N_reads,IMAGE_WIDTH,6)) # encoding of 6 flags, different for all reads\n",
    "\n",
    "#loop over flags of all reads\n",
    "for read_idx, (_,_,_,flag) in enumerate(aligned_reads):\n",
    "    flags_reads[read_idx,:,0]=flag&0x2   #each segment properly aligned according to the aligner\n",
    "    flags_reads[read_idx,:,1]=flag&0x8   #next segment unmapped\n",
    "    flags_reads[read_idx,:,2]=flag&0x10  #SEQ being reverse complemented\n",
    "    flags_reads[read_idx,:,3]=flag&0x20  #SEQ of the next segment in the template being reverse complemented\n",
    "    flags_reads[read_idx,:,4]=flag&0x100 #secondary alignment\n",
    "    flags_reads[read_idx,:,5]=flag&0x800 #supplementary alignment\n",
    "    flags_reads[read_idx,:] = flags_reads[read_idx,:]>0 #to boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize reference channels (they are the same for all reads)\n",
    "\n",
    "N_crop = 50 #bases to omit on each side of the variant position, to make lines shorter\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "nucl_colors=np.array([(255,255,255), (228,26,28), (55,126,184), (77,175,74), (152,78,163)])/255. #colors for NACTG\n",
    "nucl_cmap = ListedColormap(nucl_colors)\n",
    "\n",
    "ref_im = np.transpose(one_hot_ref[0, N_crop:-N_crop,:]).copy().astype(np.ubyte) #reference tensor is the same for all reads, we choose read 0\n",
    "\n",
    "for base_idx in range(4):\n",
    "    ref_im[base_idx,:]=ref_im[base_idx,:]*(base_idx+1) #color corresponds to the base index  \n",
    "\n",
    "ref_letters = decode_bases(ref_bases) #seqence decoding:map digits back to letters\n",
    "\n",
    "pcolor(ref_im, yticklabels=['A','C','T','G'], xticklabels=ref_letters[N_crop:-N_crop], cmap=nucl_cmap, figsize = (15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize channels specific to a given read\n",
    "\n",
    "read_number = 1\n",
    "\n",
    "N_crop = 50 #bases to omit on each side of the variant position, to make lines shorter\n",
    "\n",
    "reads_letters = [list(map(lambda x:decode_bases(x), x)) for x in reads_im[:,0,:]] #seqence decoding:map digits back to letters\n",
    "\n",
    "phreads_im = np.transpose(p_hot_reads[read_number,N_crop:-N_crop,:]).copy() \n",
    "pcolor(phreads_im, yticklabels=['A','C','T','G'], xticklabels=reads_letters[read_number][N_crop:-N_crop], cmap='Greens',figsize = (15,15))\n",
    "\n",
    "flags_im = np.transpose(flags_reads[read_number,N_crop:-N_crop,:])\n",
    "pcolor(flags_im, yticklabels=['0x2','0x8','0x10','0x20','0x100', '0x800'], xticklabels=reads_letters[read_number][N_crop:-N_crop], cmap='binary',figsize = (15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much memory does one image take?\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('test.img', 'wb') as f:\n",
    "    pickle.dump({'one_hot_ref':one_hot_ref.astype(bool), 'p_hot_reads':p_hot_reads, 'flags_reads':flags_reads.astype(bool)},f)\n",
    "    \n",
    "!ls -alh test.img"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
